{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE5zFqsorCCC",
        "outputId": "165fd950-f070-4e20-8f39-94731f38bc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npKeDRTPqR28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ddc924-c706-4c6f-d405-1c30f9cd3d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import string\n",
        "import fasttext\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the creator dataset\n",
        "creator_data = pd.read_csv('/content/drive/MyDrive/Tiktok_Project/cleaned_creator_dataset.csv')\n",
        "\n",
        "# Load the fastText model\n",
        "model = fasttext.load_model('/content/drive/MyDrive/Tiktok_Project/cc.en.300.bin')\n",
        "\n",
        "# Preprocess Text Data\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters, punctuation, and any other noise\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    # Tokenize the text into individual words or tokens\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "creator_data['Processed_Content'] = creator_data['Content'].apply(preprocess_text)\n",
        "\n",
        "# Perform Keyword Extraction\n",
        "def extract_keywords(tokens, num_keywords=5):\n",
        "    vectorizer = CountVectorizer()\n",
        "    vec = vectorizer.fit_transform([' '.join(tokens)])\n",
        "    keywords = [vectorizer.get_feature_names_out()[i] for i in vec.sum(axis=0).argsort()[0, -num_keywords:]]\n",
        "    return keywords\n",
        "\n",
        "creator_data['Keywords'] = creator_data['Processed_Content'].apply(extract_keywords)\n",
        "\n",
        "# Text Classification or Topic Modeling\n",
        "def classify_content(tokens, num_topics=3):\n",
        "    vectorizer = CountVectorizer()\n",
        "    vec = vectorizer.fit_transform([' '.join(tokens)])\n",
        "    lda = LatentDirichletAllocation(n_components=num_topics)\n",
        "    lda.fit(vec)\n",
        "    topics = lda.components_.argsort(axis=1)[:, -1]\n",
        "    topic_keywords = [vectorizer.get_feature_names_out()[topic] for topic in topics]\n",
        "    return topic_keywords\n",
        "\n",
        "creator_data['Topics'] = creator_data['Processed_Content'].apply(classify_content)\n",
        "\n",
        "# Calculate Content Similarity\n",
        "def calculate_similarity(tokens, predefined_categories):\n",
        "    content_vector = sum([model.get_word_vector(token) for token in tokens]) / len(tokens) if tokens else None\n",
        "    similarity_scores = {}\n",
        "    for category in predefined_categories:\n",
        "        category_vector = model.get_word_vector(category)\n",
        "        similarity = np.dot(content_vector, category_vector) / (np.linalg.norm(content_vector) * np.linalg.norm(category_vector))\n",
        "        similarity_scores[category] = similarity\n",
        "    return similarity_scores\n",
        "\n",
        "predefined_categories = ['dance videos', 'comedy sketches', 'makeup tutorials', 'lifestyle', 'lip syncing',\n",
        "                          'beatboxing', 'vlogs', 'parodies', 'magic tricks', 'video editing', 'music',\n",
        "                          'reaction videos', 'beauty content']\n",
        "\n",
        "creator_data['Similarity_Scores'] = creator_data['Processed_Content'].apply(lambda tokens: calculate_similarity(tokens, predefined_categories))\n",
        "\n",
        "# Save the results for further use\n",
        "creator_data[['Content', 'Processed_Content', 'Keywords', 'Topics', 'Similarity_Scores']].to_csv('content_analysis_results.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpGdhaHdwJPI",
        "outputId": "970c3d4b-dad8-49cd-be47-10aa22c95627"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-17 04:28:53.470543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-17 04:28:56.068988: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-md==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the creator dataset\n",
        "creator_data = pd.read_csv('/content/drive/MyDrive/Tiktok_Project/cleaned_creator_dataset.csv')\n",
        "\n",
        "# Load spacy model\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "creator_data['Processed_Content'] = creator_data['Content'].apply(preprocess_text)\n",
        "\n",
        "# Get vector representation of content using spacy\n",
        "def get_vector(text):\n",
        "    return nlp(text).vector\n",
        "\n",
        "creator_data['Content_Vector'] = creator_data['Processed_Content'].apply(get_vector)\n",
        "\n",
        "# Predefined categories and their vector representations\n",
        "categories = ['dance videos', 'comedy sketches', 'makeup tutorials', 'lifestyle', 'lip syncing',\n",
        "                          'beatboxing', 'vlogs', 'parodies', 'magic tricks', 'video editing', 'music',\n",
        "                          'reaction videos', 'beauty content']\n",
        "category_vectors = {category: get_vector(category) for category in categories}\n",
        "\n",
        "# Calculate similarity scores between each creator's content and predefined categories\n",
        "similarity_scores = {}\n",
        "for category, category_vector in category_vectors.items():\n",
        "    similarity_scores[category] = creator_data['Content_Vector'].apply(lambda x: cosine_similarity([x], [category_vector])[0][0])\n",
        "\n",
        "similarity_df = pd.DataFrame(similarity_scores)\n",
        "similarity_df['Creator Name'] = creator_data['Creator Name']\n",
        "similarity_df.set_index('Creator Name', inplace=True)\n",
        "\n",
        "# Save the similarity scores for further use\n",
        "similarity_df.to_csv('content_similarity_scores.csv')\n",
        "\n",
        "print(\"Content similarity scores saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUSY7VW8r1Vy",
        "outputId": "5c31ad4f-a440-49ef-bf61-200ab3f22326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content similarity scores saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "from moviepy.editor import VideoFileClip\n",
        "import cv2\n",
        "\n",
        "creator_data = pd.read_csv('/content/drive/MyDrive/Tiktok_Project/cleaned_creator_dataset.csv')\n",
        "\n",
        "base_model = InceptionV3(weights='imagenet')\n",
        "model = Model(inputs=base_model.input, outputs=base_model.get_layer('avg_pool').output)\n",
        "\n",
        "def extract_features_from_video(video_path):\n",
        "    clip = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "    for frame in clip.iter_frames(fps=1, dtype='uint8'):\n",
        "        resized_frame = cv2.resize(frame, (299, 299))\n",
        "        frames.append(resized_frame)\n",
        "    frames = np.array(frames)\n",
        "    frames = preprocess_input(frames)\n",
        "    features = model.predict(frames)\n",
        "    mean_features = np.mean(features, axis=0)\n",
        "    return mean_features\n",
        "\n",
        "def process_videos(video_paths):\n",
        "    features = []\n",
        "    for video_path in video_paths:\n",
        "        feature = extract_features_from_video(video_path)\n",
        "        features.append(feature)\n",
        "    return features\n",
        "\n",
        "creator_data['Video URL'] = creator_data['Video URL'].apply(lambda x: eval(x)) # Convert string to list\n",
        "creator_data['Visual_Features'] = creator_data['Video URL'].apply(process_videos)\n",
        "\n",
        "creator_data.to_csv('/content/drive/MyDrive/Tiktok_Project/Tiktok_Data_With_Features.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7dg9GDPgc8L",
        "outputId": "6e2c50ab-48af-4925-dcd0-e61c071beaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 7s 7s/step\n",
            "2/2 [==============================] - 12s 461ms/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 9s 9s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Tiktok_Project/Tiktok Videos/addisonrae3.mp4, 1769472 bytes wanted but 0 bytes read,at frame 420/425, at time 14.00/14.14 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Tiktok_Project/Tiktok Videos/bellapoarch2.mp4, 1769472 bytes wanted but 0 bytes read,at frame 1050/1052, at time 35.00/35.06 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 11s 822ms/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "2/2 [==============================] - 16s 4s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "2/2 [==============================] - 13s 431ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Tiktok_Project/Tiktok Videos/dixiedmelio1.mp4, 1769472 bytes wanted but 0 bytes read,at frame 1678/1680, at time 69.99/70.03 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 25s 6s/step\n",
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Tiktok_Project/Tiktok Videos/dixiedmelio3.mp4, 1769472 bytes wanted but 0 bytes read,at frame 2820/2822, at time 47.00/47.02 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 14s 3s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "5/5 [==============================] - 42s 8s/step\n",
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 5s 5s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Tiktok_Project/Tiktok Videos/jasonderulo2.mp4, 1769472 bytes wanted but 0 bytes read,at frame 720/721, at time 24.00/24.03 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 10s 10s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "3/3 [==============================] - 23s 5s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Tiktok_Project/Tiktok Videos/noahbeck1.mp4, 1769472 bytes wanted but 0 bytes read,at frame 929/934, at time 31.00/31.16 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 11s 11s/step\n",
            "1/1 [==============================] - 5s 5s/step\n",
            "2/2 [==============================] - 15s 8s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "4/4 [==============================] - 32s 7s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "2/2 [==============================] - 21s 8s/step\n",
            "1/1 [==============================] - 8s 8s/step\n",
            "1/1 [==============================] - 7s 7s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "2/2 [==============================] - 14s 3s/step\n",
            "1/1 [==============================] - 4s 4s/step\n"
          ]
        }
      ]
    }
  ]
}